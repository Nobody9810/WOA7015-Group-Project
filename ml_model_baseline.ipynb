{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5456cef4-aa51-43c7-bd12-d28ed70be03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow>=2.10.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow>=2.10.0) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (1.26.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.10.0) (3.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from opencv-python) (1.26.2)\n",
      "Requirement already satisfied: imageio in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (2.33.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from imageio) (1.26.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from imageio) (10.0.1)\n",
      "Requirement already satisfied: mlxtend in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from mlxtend) (1.11.4)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from mlxtend) (1.26.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from mlxtend) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from mlxtend) (1.3.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from mlxtend) (3.8.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (6.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.0->mlxtend) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hilma\\onedrive\\desktop\\advanced_ml_final_exam\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.10\n",
    "!pip install -U \"tensorflow>=2.10.0\"\n",
    "!pip install opencv-python\n",
    "!pip install -q git+https://github.com/tensorflow/docs\n",
    "!pip install imageio\n",
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da69a2de-0e01-48db-bb1e-c7c84bfa925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hilma\\OneDrive\\Desktop\\Advanced_ML_Final_Exam\\env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED']='0'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import cv2 # process video files\n",
    "\n",
    "import imageio\n",
    "from tensorflow_docs.vis import embed #for embedding data in a Jupyter notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000bed86-2dca-4dcd-8856-40fd173953e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hilma\\AppData\\Local\\Temp\\ipykernel_23700\\2065722346.py:10: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reset_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.compat.v1.set_random_seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    tf.keras.utils.set_random_seed(seed_value)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05073688-bf14-4e51-adf2-92262714b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder path that you store the video file and csv file\n",
    "dataset_file_path = \"C:/Users/hilma/Downloads/\"\n",
    "\n",
    "dataset_filepath = dataset_file_path+\"input_data/input_data_new_ori.csv\"\n",
    "df = pd.read_csv(dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82db73f2-27a4-4908-bce2-70ff165af858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "df[\"start_time_seconds\"]=df[\"Start_Time\"].apply(convert_time_to_seconds)\n",
    "df[\"end_time_seconds\"]=df[\"End_Time\"].apply(convert_time_to_seconds)\n",
    "class_list = df.Movement_Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f876f42-da41-4168-8076-2500f623cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = df.File_Path.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d7162c5-dfbc-4956-8b7b-008c8ee01f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['input_data/2.mp4', 'input_data/3.mp4', 'input_data/4.mp4',\n",
       "       'input_data/5.mp4', 'input_data/8.mp4', 'input_data/10.mp4',\n",
       "       'input_data/13.mp4', 'input_data/14.mp4', 'input_data/15.mp4',\n",
       "       'input_data/16.mp4', 'input_data/18.mp4', 'input_data/19.mp4',\n",
       "       'input_data/20.mp4', 'input_data/21.mp4', 'input_data/22.mp4',\n",
       "       'input_data/24.mp4', 'input_data/25.mp4', 'input_data/26.mp4',\n",
       "       'input_data/27.mp4', 'input_data/28.mp4', 'input_data/29.mp4',\n",
       "       'input_data/30.mp4', 'input_data/31.mp4', 'input_data/32.mp4',\n",
       "       'input_data/33.mp4', 'input_data/34.mp4', 'input_data/35.mp4'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71cc76b2-151b-4291-9178-1279359f3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set a specific seed value (for example, 42)\n",
    "random.seed(42)\n",
    "\n",
    "train_split_ratio = 0.8\n",
    "\n",
    "random.shuffle(video_list)\n",
    "\n",
    "split_idx = int(len(video_list) * train_split_ratio)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_videos_list_init = video_list[:split_idx]\n",
    "test_videos_list = video_list[split_idx:]\n",
    "\n",
    "\n",
    "train_validate_ratio = 0.75\n",
    "random.shuffle(train_videos_list_init)\n",
    "\n",
    "split_idx = int(len(train_videos_list_init) * train_validate_ratio)\n",
    "\n",
    "train_videos_list= train_videos_list_init[:split_idx]\n",
    "validate_videos_list = train_videos_list_init[split_idx:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3de2c285-889a-4e6a-a10c-14c28c7942bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['input_data/3.mp4', 'input_data/18.mp4', 'input_data/30.mp4',\n",
       "       'input_data/27.mp4', 'input_data/33.mp4', 'input_data/8.mp4',\n",
       "       'input_data/4.mp4', 'input_data/31.mp4', 'input_data/10.mp4',\n",
       "       'input_data/21.mp4', 'input_data/20.mp4', 'input_data/26.mp4',\n",
       "       'input_data/35.mp4', 'input_data/16.mp4', 'input_data/28.mp4'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_videos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e209b26-ce3c-4db4-85c1-9cd067d363dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['input_data/19.mp4', 'input_data/22.mp4', 'input_data/13.mp4',\n",
       "       'input_data/25.mp4', 'input_data/24.mp4', 'input_data/34.mp4'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_videos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eb3e88a-ee4f-4cb2-865a-d8c607259d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['input_data/14.mp4', 'input_data/15.mp4', 'input_data/32.mp4',\n",
       "       'input_data/2.mp4', 'input_data/5.mp4', 'input_data/29.mp4'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_videos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "984e2320-2e92-4bfb-bb21-e851352c3f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract frame from video\n",
    "def extract_frame(path,start_time,end_time):\n",
    "  frame_list = []\n",
    "  SEQUENCE_LENGTH = 24\n",
    "\n",
    "  # Open the video file for reading\n",
    "  video_reader = cv2.VideoCapture(path)\n",
    "\n",
    "  # Get the frames per second (fps), start_frame, and end_frame for the specified video duration\n",
    "  fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "  frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "  start_frame = int(start_time*fps)\n",
    "  end_frame = int(end_time*fps)\n",
    "  interval_frame = end_frame - start_frame\n",
    "\n",
    "  #calculate the skip frame number formula total duration of interval frame / sequence length\n",
    "  skip_frame_window = max(interval_frame/SEQUENCE_LENGTH,1)\n",
    "\n",
    "  # Get the height and width of the video frames\n",
    "  height = video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "  width = video_reader.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "  print(f'fps:{fps}, frame_cnt:{frame_count} interval_frame:{interval_frame}, height:{height}, width:{width}, start_frame:{start_frame},end_frame:{end_frame},skip:{skip_frame_window}')\n",
    "\n",
    "\n",
    "  # Set the video reader to the start frame\n",
    "  video_reader.set(cv2.CAP_PROP_POS_FRAMES,start_frame)\n",
    "  current_frame = start_frame\n",
    "\n",
    "  # Iterate through frames in the specified interval\n",
    "  while video_reader.isOpened() and math.ceil(current_frame) < end_frame:\n",
    "    ret, frame = video_reader.read()\n",
    "    if not ret:\n",
    "      break\n",
    "\n",
    "    # Convert the frame from BGR to RGB\n",
    "    frame_rgb_opencv = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize the frame to a specific size (720x1280)\n",
    "    resized_frame = cv2.resize(frame_rgb_opencv,(150,150))\n",
    "      \n",
    "    #normalize data to [0,1]\n",
    "    frame = tf.image.convert_image_dtype(resized_frame, tf.float32)\n",
    "\n",
    "    #brightness_increase = 70/255\n",
    "    #brightened_frame = np.where((1 - frame) < brightness_increase, 1, frame + brightness_increase)\n",
    "      \n",
    "    # Append the resized frame to the frame_list\n",
    "    frame_list.append(frame)\n",
    "\n",
    "    # Set the video reader to the next frame position\n",
    "    video_reader.set(cv2.CAP_PROP_POS_FRAMES, int(current_frame + skip_frame_window) )\n",
    "\n",
    "    # Increment the current_frame by skip_frame_window\n",
    "    current_frame += skip_frame_window\n",
    "\n",
    "\n",
    "  # Release the video reader and close any remaining OpenCV windows\n",
    "  video_reader.release()\n",
    "  cv2.destroyAllWindows()\n",
    "    \n",
    "  # Return the list of extracted frames\n",
    "  return frame_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d756c4-930b-43b7-b65a-976b5bb92594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee3ecc4-451e-4039-a842-d27e978efdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def extract_features_from_video(path, start_time, end_time, movement_name):\n",
    "    frames = extract_frame(path, start_time, end_time)  # Assuming this returns a list of frames\n",
    "    sift = cv2.SIFT_create(contrastThreshold=0.04, edgeThreshold=10.0)  # Adjust SIFT parameters\n",
    "    descriptors = []\n",
    "    descriptions = []  # To store descriptions\n",
    "    for frame in frames:\n",
    "        # Convert tensor to numpy array if it's not already\n",
    "        if isinstance(frame, tf.Tensor):\n",
    "            frame = frame.numpy()\n",
    "        \n",
    "        # Resize the frame to a smaller resolution\n",
    "        frame = cv2.resize(frame, (204, 360))  # Can adjust the resolution\n",
    "        \n",
    "        # Normalize frame intensity\n",
    "        frame = cv2.normalize(frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        gray_frame = cv2.cvtColor(frame.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        kp, des = sift.detectAndCompute(gray_frame, None)\n",
    "        if des is not None:\n",
    "            descriptors.extend(des)\n",
    "            descriptions.extend([movement_name] * len(des))  # Replicate the movement name\n",
    "    return np.array(descriptors), descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa561ee1-9bb9-4a39-892f-acb24c9acf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_extract_features(video_paths, start_times, end_times, labels):\n",
    "    all_descriptors = []\n",
    "    all_descriptions = []  # To store descriptions\n",
    "    y_data = np.array(labels)  # Convert labels to a numpy array\n",
    "\n",
    "    for i, path in enumerate(video_paths):\n",
    "        movement_name = labels[i]  # Get the 'Movement_Name' from your dataset\n",
    "        des, descriptions = extract_features_from_video(path, start_times[i], end_times[i], movement_name)\n",
    "        if des.size > 0:\n",
    "            all_descriptors.append(des)\n",
    "            all_descriptions.extend(descriptions)  # Extend the descriptions list\n",
    "    indices_to_delete = [i for i, des in enumerate(all_descriptors) if des.size == 0]\n",
    "\n",
    "    # Assuming descriptor_size is known\n",
    "    descriptor_size = all_descriptors[0].shape[1] if all_descriptors else None\n",
    "\n",
    "    if descriptor_size is None:\n",
    "        raise ValueError(\"No descriptors found in any of the videos\")\n",
    "\n",
    "    # Concatenate descriptors only if there are any\n",
    "    if all_descriptors:\n",
    "        descriptors_concatenated = np.vstack(all_descriptors)\n",
    "    else:\n",
    "        descriptors_concatenated = np.empty((0, descriptor_size))  # Empty array\n",
    "\n",
    "    # Delete the labels and descriptions for videos with no descriptors\n",
    "    y_data = np.delete(y_data, indices_to_delete)\n",
    "    all_descriptions = [all_descriptions[i] for i in range(len(all_descriptions)) if i not in indices_to_delete]\n",
    "\n",
    "    return descriptors_concatenated, y_data, all_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b837f2-6f3e-4278-9f93-58565aa39c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_video_data():\n",
    "    # Modify these paths and file names to match your dataset\n",
    "    dataset_file_path = \"C:/Users/hilma/Downloads/\"\n",
    "    dataset_filepath = dataset_file_path + \"input_data/input_data_baseline_model.csv\"\n",
    "    df = pd.read_csv(dataset_filepath)\n",
    "\n",
    "    # Assuming your DataFrame has columns 'File_Path', 'Start_Time', and 'End_Time', and 'Movement_Name'\n",
    "    video_paths = df['File_Path'].apply(lambda x: dataset_file_path + x).tolist()\n",
    "    start_times = df['Start_Time'].apply(convert_time_to_seconds).tolist()  # Assuming this function is already defined\n",
    "    end_times = df['End_Time'].apply(convert_time_to_seconds).tolist()\n",
    "    labels = df['Movement_Name'].tolist()  # Or however you wish to encode your labels\n",
    "\n",
    "    return video_paths, start_times, end_times, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a430bc3-ee39-4b02-9828-65c14a0f71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans  # Import KMeans\n",
    "\n",
    "def create_bag_of_words(descriptors, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=5)  # Specify n_init here\n",
    "    kmeans.fit(descriptors)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f236cf-c485-4225-9548-3089c9baf200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def generate_histogram(features, kmeans_model, num_clusters): \n",
    "    labels = kmeans_model.predict(features)\n",
    "    label_counts = Counter(labels)\n",
    "    histogram = np.array([label_counts.get(i, 0) for i in range(num_clusters)])\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0e566e0-7747-473e-9a7e-59a677d28319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videos_to_histograms(video_data, kmeans_model, num_clusters):  # Added num_clusters as a parameter\n",
    "    histograms = []\n",
    "    for path, start, end, label in video_data:\n",
    "        features, _ = extract_features_from_video(path, start, end, label)\n",
    "        histogram = generate_histogram(features, kmeans_model, num_clusters)  # Pass num_clusters here\n",
    "        histograms.append(histogram)\n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d5a724-b341-46d2-838e-74fd724e024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading video data...\n",
      "Step 1 completed: Video data loaded.\n",
      "Step 2: Extracting features and labels...\n",
      "fps:15, frame_cnt:931 interval_frame:75, height:480.0, width:640.0, start_frame:330,end_frame:405,skip:3.125\n",
      "fps:30, frame_cnt:5991 interval_frame:210, height:720.0, width:1280.0, start_frame:1950,end_frame:2160,skip:8.75\n",
      "fps:30, frame_cnt:5991 interval_frame:240, height:720.0, width:1280.0, start_frame:3960,end_frame:4200,skip:10.0\n",
      "fps:23, frame_cnt:6078 interval_frame:69, height:240.0, width:320.0, start_frame:414,end_frame:483,skip:2.875\n",
      "fps:23, frame_cnt:6078 interval_frame:92, height:240.0, width:320.0, start_frame:1081,end_frame:1173,skip:3.8333333333333335\n",
      "fps:23, frame_cnt:6078 interval_frame:161, height:240.0, width:320.0, start_frame:1610,end_frame:1771,skip:6.708333333333333\n",
      "fps:23, frame_cnt:6078 interval_frame:414, height:240.0, width:320.0, start_frame:2070,end_frame:2484,skip:17.25\n",
      "fps:23, frame_cnt:6078 interval_frame:46, height:240.0, width:320.0, start_frame:2829,end_frame:2875,skip:1.9166666666666667\n",
      "fps:23, frame_cnt:6078 interval_frame:161, height:240.0, width:320.0, start_frame:2944,end_frame:3105,skip:6.708333333333333\n",
      "fps:23, frame_cnt:6078 interval_frame:115, height:240.0, width:320.0, start_frame:4508,end_frame:4623,skip:4.791666666666667\n",
      "fps:23, frame_cnt:6078 interval_frame:92, height:240.0, width:320.0, start_frame:5060,end_frame:5152,skip:3.8333333333333335\n",
      "fps:23, frame_cnt:3213 interval_frame:207, height:240.0, width:320.0, start_frame:414,end_frame:621,skip:8.625\n",
      "fps:23, frame_cnt:3213 interval_frame:92, height:240.0, width:320.0, start_frame:759,end_frame:851,skip:3.8333333333333335\n",
      "fps:23, frame_cnt:3213 interval_frame:161, height:240.0, width:320.0, start_frame:1150,end_frame:1311,skip:6.708333333333333\n",
      "fps:23, frame_cnt:3213 interval_frame:391, height:240.0, width:320.0, start_frame:1564,end_frame:1955,skip:16.291666666666668\n",
      "fps:30, frame_cnt:1603 interval_frame:420, height:1080.0, width:608.0, start_frame:960,end_frame:1380,skip:17.5\n",
      "fps:30, frame_cnt:2652 interval_frame:120, height:360.0, width:480.0, start_frame:90,end_frame:210,skip:5.0\n",
      "fps:30, frame_cnt:2652 interval_frame:300, height:360.0, width:480.0, start_frame:1200,end_frame:1500,skip:12.5\n",
      "fps:29, frame_cnt:3316 interval_frame:116, height:240.0, width:320.0, start_frame:2726,end_frame:2842,skip:4.833333333333333\n",
      "fps:30, frame_cnt:670 interval_frame:270, height:480.0, width:640.0, start_frame:210,end_frame:480,skip:11.25\n",
      "fps:30, frame_cnt:4281 interval_frame:90, height:320.0, width:568.0, start_frame:1140,end_frame:1230,skip:3.75\n",
      "fps:30, frame_cnt:4281 interval_frame:90, height:320.0, width:568.0, start_frame:1980,end_frame:2070,skip:3.75\n",
      "fps:30, frame_cnt:4281 interval_frame:60, height:320.0, width:568.0, start_frame:2940,end_frame:3000,skip:2.5\n",
      "fps:30, frame_cnt:4281 interval_frame:120, height:320.0, width:568.0, start_frame:3330,end_frame:3450,skip:5.0\n",
      "fps:30, frame_cnt:4281 interval_frame:60, height:320.0, width:568.0, start_frame:4080,end_frame:4140,skip:2.5\n",
      "fps:30, frame_cnt:611 interval_frame:210, height:720.0, width:1280.0, start_frame:90,end_frame:300,skip:8.75\n",
      "fps:29, frame_cnt:363 interval_frame:232, height:720.0, width:1280.0, start_frame:116,end_frame:348,skip:9.666666666666666\n",
      "fps:28, frame_cnt:945 interval_frame:56, height:1080.0, width:608.0, start_frame:196,end_frame:252,skip:2.3333333333333335\n",
      "fps:28, frame_cnt:945 interval_frame:84, height:1080.0, width:608.0, start_frame:392,end_frame:476,skip:3.5\n",
      "fps:28, frame_cnt:945 interval_frame:28, height:1080.0, width:608.0, start_frame:532,end_frame:560,skip:1.1666666666666667\n",
      "fps:24, frame_cnt:449 interval_frame:192, height:360.0, width:480.0, start_frame:72,end_frame:264,skip:8.0\n",
      "fps:24, frame_cnt:449 interval_frame:48, height:360.0, width:480.0, start_frame:360,end_frame:408,skip:2.0\n",
      "fps:30, frame_cnt:3671 interval_frame:150, height:720.0, width:1280.0, start_frame:0,end_frame:150,skip:6.25\n",
      "fps:30, frame_cnt:3671 interval_frame:60, height:720.0, width:1280.0, start_frame:480,end_frame:540,skip:2.5\n",
      "fps:30, frame_cnt:3671 interval_frame:30, height:720.0, width:1280.0, start_frame:720,end_frame:750,skip:1.25\n",
      "fps:30, frame_cnt:3671 interval_frame:60, height:720.0, width:1280.0, start_frame:900,end_frame:960,skip:2.5\n",
      "fps:30, frame_cnt:3671 interval_frame:120, height:720.0, width:1280.0, start_frame:1170,end_frame:1290,skip:5.0\n",
      "fps:30, frame_cnt:3671 interval_frame:150, height:720.0, width:1280.0, start_frame:2100,end_frame:2250,skip:6.25\n",
      "fps:30, frame_cnt:3671 interval_frame:90, height:720.0, width:1280.0, start_frame:2670,end_frame:2760,skip:3.75\n",
      "fps:30, frame_cnt:3671 interval_frame:30, height:720.0, width:1280.0, start_frame:2970,end_frame:3000,skip:1.25\n",
      "fps:30, frame_cnt:1432 interval_frame:30, height:720.0, width:1280.0, start_frame:210,end_frame:240,skip:1.25\n",
      "fps:30, frame_cnt:1432 interval_frame:120, height:720.0, width:1280.0, start_frame:420,end_frame:540,skip:5.0\n",
      "fps:30, frame_cnt:1432 interval_frame:60, height:720.0, width:1280.0, start_frame:1020,end_frame:1080,skip:2.5\n",
      "fps:30, frame_cnt:1538 interval_frame:150, height:360.0, width:204.0, start_frame:330,end_frame:480,skip:6.25\n",
      "fps:30, frame_cnt:1538 interval_frame:120, height:360.0, width:204.0, start_frame:570,end_frame:690,skip:5.0\n",
      "fps:30, frame_cnt:1538 interval_frame:150, height:360.0, width:204.0, start_frame:840,end_frame:990,skip:6.25\n",
      "fps:15, frame_cnt:526 interval_frame:180, height:720.0, width:1280.0, start_frame:30,end_frame:210,skip:7.5\n",
      "fps:15, frame_cnt:842 interval_frame:60, height:240.0, width:320.0, start_frame:45,end_frame:105,skip:2.5\n",
      "fps:30, frame_cnt:1146 interval_frame:180, height:720.0, width:1280.0, start_frame:510,end_frame:690,skip:7.5\n",
      "fps:30, frame_cnt:1146 interval_frame:150, height:720.0, width:1280.0, start_frame:960,end_frame:1110,skip:6.25\n",
      "fps:24, frame_cnt:2633 interval_frame:48, height:360.0, width:480.0, start_frame:24,end_frame:72,skip:2.0\n",
      "fps:24, frame_cnt:2633 interval_frame:24, height:360.0, width:480.0, start_frame:144,end_frame:168,skip:1.0\n",
      "fps:24, frame_cnt:2633 interval_frame:48, height:360.0, width:480.0, start_frame:288,end_frame:336,skip:2.0\n",
      "fps:24, frame_cnt:2633 interval_frame:120, height:360.0, width:480.0, start_frame:432,end_frame:552,skip:5.0\n",
      "fps:24, frame_cnt:2633 interval_frame:72, height:360.0, width:480.0, start_frame:624,end_frame:696,skip:3.0\n",
      "fps:24, frame_cnt:2633 interval_frame:48, height:360.0, width:480.0, start_frame:1200,end_frame:1248,skip:2.0\n",
      "fps:24, frame_cnt:2633 interval_frame:48, height:360.0, width:480.0, start_frame:1680,end_frame:1728,skip:2.0\n",
      "fps:24, frame_cnt:746 interval_frame:168, height:360.0, width:202.0, start_frame:48,end_frame:216,skip:7.0\n",
      "fps:24, frame_cnt:746 interval_frame:192, height:360.0, width:202.0, start_frame:432,end_frame:624,skip:8.0\n",
      "Step 2 completed: Features and labels extracted.\n",
      "Step 3: Creating Bag of Words model with 50 clusters...\n",
      "Step 3 completed: Bag of Words model created.\n",
      "Step 4: Converting videos to histograms of visual words...\n",
      "fps:15, frame_cnt:931 interval_frame:75, height:480.0, width:640.0, start_frame:330,end_frame:405,skip:3.125\n",
      "fps:30, frame_cnt:5991 interval_frame:210, height:720.0, width:1280.0, start_frame:1950,end_frame:2160,skip:8.75\n",
      "fps:30, frame_cnt:5991 interval_frame:240, height:720.0, width:1280.0, start_frame:3960,end_frame:4200,skip:10.0\n",
      "fps:23, frame_cnt:6078 interval_frame:69, height:240.0, width:320.0, start_frame:414,end_frame:483,skip:2.875\n",
      "fps:23, frame_cnt:6078 interval_frame:92, height:240.0, width:320.0, start_frame:1081,end_frame:1173,skip:3.8333333333333335\n",
      "fps:23, frame_cnt:6078 interval_frame:161, height:240.0, width:320.0, start_frame:1610,end_frame:1771,skip:6.708333333333333\n",
      "fps:23, frame_cnt:6078 interval_frame:414, height:240.0, width:320.0, start_frame:2070,end_frame:2484,skip:17.25\n",
      "fps:23, frame_cnt:6078 interval_frame:46, height:240.0, width:320.0, start_frame:2829,end_frame:2875,skip:1.9166666666666667\n",
      "fps:23, frame_cnt:6078 interval_frame:161, height:240.0, width:320.0, start_frame:2944,end_frame:3105,skip:6.708333333333333\n",
      "fps:23, frame_cnt:6078 interval_frame:115, height:240.0, width:320.0, start_frame:4508,end_frame:4623,skip:4.791666666666667\n",
      "fps:23, frame_cnt:6078 interval_frame:92, height:240.0, width:320.0, start_frame:5060,end_frame:5152,skip:3.8333333333333335\n",
      "fps:23, frame_cnt:3213 interval_frame:207, height:240.0, width:320.0, start_frame:414,end_frame:621,skip:8.625\n",
      "fps:23, frame_cnt:3213 interval_frame:92, height:240.0, width:320.0, start_frame:759,end_frame:851,skip:3.8333333333333335\n",
      "fps:23, frame_cnt:3213 interval_frame:161, height:240.0, width:320.0, start_frame:1150,end_frame:1311,skip:6.708333333333333\n",
      "fps:23, frame_cnt:3213 interval_frame:391, height:240.0, width:320.0, start_frame:1564,end_frame:1955,skip:16.291666666666668\n",
      "fps:30, frame_cnt:1603 interval_frame:420, height:1080.0, width:608.0, start_frame:960,end_frame:1380,skip:17.5\n",
      "fps:30, frame_cnt:2652 interval_frame:120, height:360.0, width:480.0, start_frame:90,end_frame:210,skip:5.0\n",
      "fps:30, frame_cnt:2652 interval_frame:300, height:360.0, width:480.0, start_frame:1200,end_frame:1500,skip:12.5\n",
      "fps:29, frame_cnt:3316 interval_frame:116, height:240.0, width:320.0, start_frame:2726,end_frame:2842,skip:4.833333333333333\n",
      "fps:30, frame_cnt:670 interval_frame:270, height:480.0, width:640.0, start_frame:210,end_frame:480,skip:11.25\n",
      "fps:30, frame_cnt:4281 interval_frame:90, height:320.0, width:568.0, start_frame:1140,end_frame:1230,skip:3.75\n",
      "fps:30, frame_cnt:4281 interval_frame:90, height:320.0, width:568.0, start_frame:1980,end_frame:2070,skip:3.75\n",
      "fps:30, frame_cnt:4281 interval_frame:60, height:320.0, width:568.0, start_frame:2940,end_frame:3000,skip:2.5\n",
      "fps:30, frame_cnt:4281 interval_frame:120, height:320.0, width:568.0, start_frame:3330,end_frame:3450,skip:5.0\n",
      "fps:30, frame_cnt:4281 interval_frame:60, height:320.0, width:568.0, start_frame:4080,end_frame:4140,skip:2.5\n",
      "fps:30, frame_cnt:611 interval_frame:210, height:720.0, width:1280.0, start_frame:90,end_frame:300,skip:8.75\n",
      "fps:29, frame_cnt:363 interval_frame:232, height:720.0, width:1280.0, start_frame:116,end_frame:348,skip:9.666666666666666\n",
      "fps:28, frame_cnt:945 interval_frame:56, height:1080.0, width:608.0, start_frame:196,end_frame:252,skip:2.3333333333333335\n",
      "fps:28, frame_cnt:945 interval_frame:84, height:1080.0, width:608.0, start_frame:392,end_frame:476,skip:3.5\n",
      "fps:28, frame_cnt:945 interval_frame:28, height:1080.0, width:608.0, start_frame:532,end_frame:560,skip:1.1666666666666667\n",
      "fps:24, frame_cnt:449 interval_frame:192, height:360.0, width:480.0, start_frame:72,end_frame:264,skip:8.0\n",
      "fps:24, frame_cnt:449 interval_frame:48, height:360.0, width:480.0, start_frame:360,end_frame:408,skip:2.0\n",
      "fps:30, frame_cnt:3671 interval_frame:150, height:720.0, width:1280.0, start_frame:0,end_frame:150,skip:6.25\n",
      "fps:30, frame_cnt:3671 interval_frame:60, height:720.0, width:1280.0, start_frame:480,end_frame:540,skip:2.5\n",
      "fps:30, frame_cnt:3671 interval_frame:30, height:720.0, width:1280.0, start_frame:720,end_frame:750,skip:1.25\n",
      "fps:30, frame_cnt:3671 interval_frame:60, height:720.0, width:1280.0, start_frame:900,end_frame:960,skip:2.5\n",
      "fps:30, frame_cnt:3671 interval_frame:120, height:720.0, width:1280.0, start_frame:1170,end_frame:1290,skip:5.0\n",
      "fps:30, frame_cnt:3671 interval_frame:150, height:720.0, width:1280.0, start_frame:2100,end_frame:2250,skip:6.25\n",
      "fps:30, frame_cnt:3671 interval_frame:90, height:720.0, width:1280.0, start_frame:2670,end_frame:2760,skip:3.75\n",
      "fps:30, frame_cnt:3671 interval_frame:30, height:720.0, width:1280.0, start_frame:2970,end_frame:3000,skip:1.25\n",
      "fps:30, frame_cnt:1432 interval_frame:30, height:720.0, width:1280.0, start_frame:210,end_frame:240,skip:1.25\n",
      "fps:30, frame_cnt:1432 interval_frame:120, height:720.0, width:1280.0, start_frame:420,end_frame:540,skip:5.0\n",
      "fps:30, frame_cnt:1432 interval_frame:60, height:720.0, width:1280.0, start_frame:1020,end_frame:1080,skip:2.5\n",
      "fps:30, frame_cnt:1538 interval_frame:150, height:360.0, width:204.0, start_frame:330,end_frame:480,skip:6.25\n",
      "fps:30, frame_cnt:1538 interval_frame:120, height:360.0, width:204.0, start_frame:570,end_frame:690,skip:5.0\n",
      "fps:30, frame_cnt:1538 interval_frame:150, height:360.0, width:204.0, start_frame:840,end_frame:990,skip:6.25\n",
      "fps:15, frame_cnt:526 interval_frame:180, height:720.0, width:1280.0, start_frame:30,end_frame:210,skip:7.5\n",
      "fps:15, frame_cnt:842 interval_frame:60, height:240.0, width:320.0, start_frame:45,end_frame:105,skip:2.5\n",
      "fps:30, frame_cnt:1146 interval_frame:180, height:720.0, width:1280.0, start_frame:510,end_frame:690,skip:7.5\n",
      "fps:30, frame_cnt:1146 interval_frame:150, height:720.0, width:1280.0, start_frame:960,end_frame:1110,skip:6.25\n",
      "fps:24, frame_cnt:2633 interval_frame:48, height:360.0, width:480.0, start_frame:24,end_frame:72,skip:2.0\n",
      "fps:24, frame_cnt:2633 interval_frame:24, height:360.0, width:480.0, start_frame:144,end_frame:168,skip:1.0\n",
      "fps:24, frame_cnt:2633 interval_frame:48, height:360.0, width:480.0, start_frame:288,end_frame:336,skip:2.0\n",
      "fps:24, frame_cnt:2633 interval_frame:120, height:360.0, width:480.0, start_frame:432,end_frame:552,skip:5.0\n",
      "fps:24, frame_cnt:2633 interval_frame:72, height:360.0, width:480.0, start_frame:624,end_frame:696,skip:3.0\n",
      "fps:24, frame_cnt:2633 interval_frame:48, height:360.0, width:480.0, start_frame:1200,end_frame:1248,skip:2.0\n",
      "fps:24, frame_cnt:2633 interval_frame:48, height:360.0, width:480.0, start_frame:1680,end_frame:1728,skip:2.0\n",
      "fps:24, frame_cnt:746 interval_frame:168, height:360.0, width:202.0, start_frame:48,end_frame:216,skip:7.0\n",
      "fps:24, frame_cnt:746 interval_frame:192, height:360.0, width:202.0, start_frame:432,end_frame:624,skip:8.0\n",
      "Step 4 completed: Videos converted to histograms.\n",
      "Step 5: Standardizing features...\n",
      "Step 5 completed: Features standardized.\n",
      "Step 6: Splitting data into training and test sets...\n",
      "Step 6 completed: Data split.\n",
      "Step 7: Initializing and training the classifier...\n",
      "Step 7 completed: Classifier trained.\n",
      "Step 8: Testing the model...\n",
      "Accuracy: 0.75\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 1 12]]\n",
      "Step 8 completed: Model tested.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "def main():\n",
    "    print(\"Step 1: Loading video data...\")\n",
    "    video_paths, start_times, end_times, labels = load_video_data()  # Implement this according to the data\n",
    "    print(\"Step 1 completed: Video data loaded.\")\n",
    "    \n",
    "    # Extract features and labels\n",
    "    print(\"Step 2: Extracting features and labels...\")\n",
    "    all_descriptors, y_data, all_descriptions = load_data_and_extract_features(video_paths, start_times, end_times, labels)\n",
    "    print(\"Step 2 completed: Features and labels extracted.\")\n",
    "    \n",
    "    # Create Bag of Words model with a fixed number of clusters\n",
    "    num_clusters = 50  # Adjust as needed for specific task\n",
    "    print(f\"Step 3: Creating Bag of Words model with {num_clusters} clusters...\")\n",
    "    # Initialize the KMeans model with an explicit n_init value to suppress the warning\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters, n_init=10)\n",
    "    # Fit the KMeans model on all descriptors\n",
    "    kmeans_model.fit(np.vstack(all_descriptors))\n",
    "    print(\"Step 3 completed: Bag of Words model created.\")\n",
    "    \n",
    "    # Convert each video to a histogram of visual words\n",
    "    print(\"Step 4: Converting videos to histograms of visual words...\")\n",
    "    X_histograms = [videos_to_histograms([(path, start, end, label)], kmeans_model, num_clusters)[0]  # Pass num_clusters here\n",
    "                    for path, start, end, label in zip(video_paths, start_times, end_times, labels)]\n",
    "    X_histograms = np.array(X_histograms)\n",
    "    print(\"Step 4 completed: Videos converted to histograms.\")\n",
    "    \n",
    "    # Standardize the features\n",
    "    print(\"Step 5: Standardizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X_histograms)\n",
    "    print(\"Step 5 completed: Features standardized.\")\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    print(\"Step 6: Splitting data into training and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_std, y_data, test_size=0.27, shuffle=False)\n",
    "    print(\"Step 6 completed: Data split.\")\n",
    "\n",
    "    # Initialize and train the classifier\n",
    "    print(\"Step 7: Initializing and training the classifier...\")\n",
    "    clf = SVC(kernel='poly', C=10)  # You can change the kernel and other parameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Step 7 completed: Classifier trained.\")\n",
    "\n",
    "    # Test the model\n",
    "    print(\"Step 8: Testing the model...\")\n",
    "    test_predictions = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    accuracy = accuracy_score(y_test, test_predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    # print(\"Test Set Classification Report:\")\n",
    "    # print(classification_report(y_test, test_predictions))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, test_predictions))\n",
    "    print(\"Step 8 completed: Model tested.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805569f-3967-4283-aeae-7d59adcdf9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
