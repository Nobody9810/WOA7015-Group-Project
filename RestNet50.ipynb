{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lNsfr8advPg3",
      "metadata": {
        "id": "lNsfr8advPg3"
      },
      "source": [
        "## Setup ( If run on non-colab environment)\n",
        "Begin by installing and importing some necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gl1HS0KLvuBJ",
      "metadata": {
        "id": "gl1HS0KLvuBJ"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python\n",
        "!pip install tensorflow\n",
        "!pip install imageio\n",
        "!pip install mlxtend"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D-6Odtz4v4nN",
      "metadata": {
        "id": "D-6Odtz4v4nN"
      },
      "source": [
        "## Import library\n",
        "Import all library which will be used later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d69e67-30e9-47c6-9808-6b754460e792",
      "metadata": {
        "id": "25d69e67-30e9-47c6-9808-6b754460e792"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTHONHASHSEED']='0'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import cv2 # process video files\n",
        "import imageio\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4fBdB6twG5i",
      "metadata": {
        "id": "g4fBdB6twG5i"
      },
      "source": [
        "Check exisitng GPU device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eWoFRGoXH_Gx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWoFRGoXH_Gx",
        "outputId": "ccde2d44-f2c6-4e71-837d-331529bfe9a5"
      },
      "outputs": [],
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs:\", len(physical_devices))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ce0cpvUMwKxf",
      "metadata": {
        "id": "Ce0cpvUMwKxf"
      },
      "source": [
        "## Set Random Seed\n",
        "Set random seed to ensure the result reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac721188-c407-4abe-8cd5-05d8b43c10b9",
      "metadata": {
        "id": "ac721188-c407-4abe-8cd5-05d8b43c10b9"
      },
      "outputs": [],
      "source": [
        "def reset_seed(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.compat.v1.set_random_seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "    tf.keras.utils.set_random_seed(seed_value)\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "from keras import backend as K\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89Do0HQlweFJ",
      "metadata": {
        "id": "89Do0HQlweFJ"
      },
      "source": [
        "## Access Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b98bcd-0a3f-4fa7-b102-82e12f19b644",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81b98bcd-0a3f-4fa7-b102-82e12f19b644",
        "outputId": "017af9d6-08e0-44b2-de67-e649e44ce5f7"
      },
      "outputs": [],
      "source": [
        "# folder path that you store the video file and csv file\n",
        "drive.mount('/content/drive')\n",
        "dataset_file_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "dataset_filepath = dataset_file_path+\"final_exam/input_data_new_2.csv\"\n",
        "df = pd.read_csv(dataset_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "213fbfdf-2439-4223-8fb6-a0982d435cbc",
      "metadata": {
        "id": "213fbfdf-2439-4223-8fb6-a0982d435cbc"
      },
      "outputs": [],
      "source": [
        "def convert_time_to_seconds(time_str):\n",
        "    h, m, s = map(int, time_str.split(':'))\n",
        "    return h * 3600 + m * 60 + s\n",
        "\n",
        "df[\"start_time_seconds\"]=df[\"Start_Time\"].apply(convert_time_to_seconds)\n",
        "df[\"end_time_seconds\"]=df[\"End_Time\"].apply(convert_time_to_seconds)\n",
        "class_list = df.Movement_Name.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfffdb14-89f2-4b6b-8ba0-7d43745d8d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfffdb14-89f2-4b6b-8ba0-7d43745d8d38",
        "outputId": "a8aef156-2eb5-461b-9ffb-95e7bdc250bd"
      },
      "outputs": [],
      "source": [
        "video_list = df.File_Path.unique()\n",
        "video_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ULXnX3SAwzVA",
      "metadata": {
        "id": "ULXnX3SAwzVA"
      },
      "source": [
        "## Video Dataset Split\n",
        "Split video into train, validation and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea571350-edf0-48eb-ad83-7d0921674580",
      "metadata": {
        "id": "ea571350-edf0-48eb-ad83-7d0921674580"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Set a specific seed value (for example, 42)\n",
        "random.seed(42)\n",
        "\n",
        "train_split_ratio = 0.8\n",
        "\n",
        "random.shuffle(video_list)\n",
        "\n",
        "split_idx = int(len(video_list) * train_split_ratio)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_videos_list_init = video_list[:split_idx]\n",
        "test_videos_list = video_list[split_idx:]\n",
        "\n",
        "\n",
        "train_validate_ratio = 0.75\n",
        "random.shuffle(train_videos_list_init)\n",
        "\n",
        "split_idx = int(len(train_videos_list_init) * train_validate_ratio)\n",
        "\n",
        "train_videos_list= train_videos_list_init[:split_idx]\n",
        "validate_videos_list = train_videos_list_init[split_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d09c054b-23db-43b8-9aaa-28bbf6c775c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d09c054b-23db-43b8-9aaa-28bbf6c775c6",
        "outputId": "6b8d51a2-620b-43c6-8186-ebe1413d6215"
      },
      "outputs": [],
      "source": [
        "train_videos_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19858695-9cad-46ea-ab5a-961adaac1a51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19858695-9cad-46ea-ab5a-961adaac1a51",
        "outputId": "15126bcb-fbef-4d1a-96a6-59e5221b972c"
      },
      "outputs": [],
      "source": [
        "validate_videos_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e8e36ac-5bf6-48e4-af0d-1c486591c7dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e8e36ac-5bf6-48e4-af0d-1c486591c7dc",
        "outputId": "83f51948-259f-41c5-a7f4-0bf3e0aec400"
      },
      "outputs": [],
      "source": [
        "test_videos_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wIJ1zdI1w_up",
      "metadata": {
        "id": "wIJ1zdI1w_up"
      },
      "source": [
        "## Extract Frame\n",
        "Extract frame and video preprocessing process (frame skipping, frame resize, frame color code conversion, color code normalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63943401-61c9-46a8-8459-701ad0b416d6",
      "metadata": {
        "id": "63943401-61c9-46a8-8459-701ad0b416d6"
      },
      "outputs": [],
      "source": [
        "#Extract frame from video\n",
        "def extract_frame(path,start_time,end_time):\n",
        "  frame_list = []\n",
        "  SEQUENCE_LENGTH = 24\n",
        "\n",
        "  # Open the video file for reading\n",
        "  video_reader = cv2.VideoCapture(path)\n",
        "\n",
        "  # Get the frames per second (fps), start_frame, and end_frame for the specified video duration\n",
        "  fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
        "  frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  start_frame = int(start_time*fps)\n",
        "  end_frame = int(end_time*fps)\n",
        "  interval_frame = end_frame - start_frame\n",
        "\n",
        "  #calculate the skip frame number formula total duration of interval frame / sequence length\n",
        "  skip_frame_window = max(interval_frame/SEQUENCE_LENGTH,1)\n",
        "\n",
        "  # Get the height and width of the video frames\n",
        "  height = video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "  width = video_reader.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "  print(f'fps:{fps}, frame_cnt:{frame_count} interval_frame:{interval_frame}, height:{height}, width:{width}, start_frame:{start_frame},end_frame:{end_frame},skip:{skip_frame_window}')\n",
        "\n",
        "\n",
        "  # Set the video reader to the start frame\n",
        "  video_reader.set(cv2.CAP_PROP_POS_FRAMES,start_frame)\n",
        "  current_frame = start_frame\n",
        "\n",
        "  # Iterate through frames in the specified interval\n",
        "  while video_reader.isOpened() and math.ceil(current_frame) < end_frame:\n",
        "    ret, frame = video_reader.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    # Convert the frame from BGR to RGB\n",
        "    frame_rgb_opencv = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize the frame to a specific size (720x1280)\n",
        "    resized_frame = cv2.resize(frame_rgb_opencv,(150,150))\n",
        "\n",
        "    #normalize data to [0,1]\n",
        "    frame = tf.image.convert_image_dtype(resized_frame, tf.float32)\n",
        "\n",
        "    #brightness_increase = 70/255\n",
        "    #brightened_frame = np.where((1 - frame) < brightness_increase, 1, frame + brightness_increase)\n",
        "\n",
        "    # Append the resized frame to the frame_list\n",
        "    frame_list.append(frame)\n",
        "\n",
        "    # Set the video reader to the next frame position\n",
        "    video_reader.set(cv2.CAP_PROP_POS_FRAMES, int(current_frame + skip_frame_window) )\n",
        "\n",
        "    # Increment the current_frame by skip_frame_window\n",
        "    current_frame += skip_frame_window\n",
        "\n",
        "\n",
        "  # Release the video reader and close any remaining OpenCV windows\n",
        "  video_reader.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "  # Return the list of extracted frames\n",
        "  return frame_list;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GXMF71iCxLko",
      "metadata": {
        "id": "GXMF71iCxLko"
      },
      "source": [
        "Extract frame and store in X_train, X_validate, X_Test. And store the relative label in Y_train, Y_validate, Y_Test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f43ada-cfe4-4d62-bc09-74da908abc76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f43ada-cfe4-4d62-bc09-74da908abc76",
        "outputId": "2cd12d1c-27fb-47ff-96ce-88b0859742e4"
      },
      "outputs": [],
      "source": [
        "reset_seed(42)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "X_train = [] #Save the frame\n",
        "Y_train = [] #Save the label\n",
        "X_validate = [] #Save the frame\n",
        "Y_validate = [] #Save the label\n",
        "X_test = [] #Save the frame\n",
        "Y_test = [] #Save the label\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "    start_time = row[\"start_time_seconds\"]\n",
        "    end_time = row[\"end_time_seconds\"]\n",
        "    path = dataset_file_path + row[\"File_Path\"] # Get the vidoe file path\n",
        "    label = row[\"Movement_Name\"] # Get the Movement lable\n",
        "    video_path = row[\"File_Path\"]\n",
        "\n",
        "    if video_path in train_videos_list:\n",
        "        Y_train.append(np.where(class_list == label)[0][0])\n",
        "        frame_list = extract_frame(path,start_time,end_time)\n",
        "        X_train.append(np.asarray(frame_list))\n",
        "    elif video_path in validate_videos_list:\n",
        "        Y_validate.append(np.where(class_list == label)[0][0])\n",
        "        frame_list = extract_frame(path,start_time,end_time)\n",
        "        X_validate.append(np.asarray(frame_list))\n",
        "    else:\n",
        "        Y_test.append(np.where(class_list == label)[0][0])\n",
        "        frame_list = extract_frame(path,start_time,end_time)\n",
        "        X_test.append(np.asarray(frame_list))\n",
        "\n",
        "#encode the label by using one-hot encoded\n",
        "one_hot_encoded_labels_train = to_categorical(Y_train)\n",
        "Y_train = one_hot_encoded_labels_train\n",
        "\n",
        "one_hot_encoded_labels_validate = to_categorical(Y_validate)\n",
        "Y_validate = one_hot_encoded_labels_validate\n",
        "\n",
        "one_hot_encoded_labels_test = to_categorical(Y_test)\n",
        "Y_test = one_hot_encoded_labels_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ux1segNcxgc3",
      "metadata": {
        "id": "Ux1segNcxgc3"
      },
      "source": [
        "## Training Data Augmentation\n",
        "Augmentation techniques include rotate, flipping, grayscale and color inverted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F2um62J4x779",
      "metadata": {
        "id": "F2um62J4x779"
      },
      "source": [
        "### Flipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ez_7pFM-xxl9",
      "metadata": {
        "id": "ez_7pFM-xxl9"
      },
      "outputs": [],
      "source": [
        "def data_augmentation_flip_bright(X_train,Y_train):\n",
        "    X_train_aug = []\n",
        "    Y_train_aug = []\n",
        "\n",
        "    for frame_list in X_train:\n",
        "        frame_list_aug = []\n",
        "        for frame in frame_list:\n",
        "            flipped_frame = cv2.flip(frame,1)\n",
        "\n",
        "            brightness_increase = 75/255\n",
        "            brightened_frame = np.where((1 - flipped_frame) < brightness_increase, 1, flipped_frame + brightness_increase)\n",
        "\n",
        "            frame_list_aug.append(flipped_frame)\n",
        "        X_train_aug.append(np.asarray(frame_list_aug))\n",
        "\n",
        "    for label in Y_train:\n",
        "        Y_train_aug.append(label)\n",
        "\n",
        "    return X_train_aug,Y_train_aug"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "csmjOR1jyEhm",
      "metadata": {
        "id": "csmjOR1jyEhm"
      },
      "source": [
        "### Rotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u6Jptc9Vx0Gy",
      "metadata": {
        "id": "u6Jptc9Vx0Gy"
      },
      "outputs": [],
      "source": [
        "def data_augmentation_rotate_contrast(X_train,Y_train):\n",
        "    X_train_aug = []\n",
        "    Y_train_aug = []\n",
        "\n",
        "    for frame_list in X_train:\n",
        "        frame_list_aug = []\n",
        "        for frame in frame_list:\n",
        "            # Define the rotation angle (in degrees)\n",
        "            angle = 180  # Example rotation angle\n",
        "\n",
        "            # Get image dimensions for rotation\n",
        "            height, width = frame.shape[:2]\n",
        "\n",
        "            # Calculate the rotation matrix\n",
        "            rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
        "\n",
        "            # Perform the rotation\n",
        "            rotated_frame = cv2.warpAffine(frame, rotation_matrix, (width, height), flags=cv2.INTER_LINEAR)\n",
        "\n",
        "\n",
        "            alpha = 1.5  # Contrast control (1.0 means no change)\n",
        "            adjusted_frame = cv2.multiply(rotated_frame, np.array([alpha]))\n",
        "\n",
        "            frame_list_aug.append(adjusted_frame)\n",
        "        X_train_aug.append(np.asarray(frame_list_aug))\n",
        "\n",
        "    for label in Y_train:\n",
        "        Y_train_aug.append(label)\n",
        "\n",
        "    return X_train_aug,Y_train_aug"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ahwzX8ayHLz",
      "metadata": {
        "id": "3ahwzX8ayHLz"
      },
      "source": [
        "### Grayscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "umUkQ2Cox27s",
      "metadata": {
        "id": "umUkQ2Cox27s"
      },
      "outputs": [],
      "source": [
        "def data_augmentation_grayscale(X_train,Y_train):\n",
        "    X_train_aug = []\n",
        "    Y_train_aug = []\n",
        "\n",
        "    for frame_list in X_train:\n",
        "        frame_list_aug = []\n",
        "        for frame in frame_list:\n",
        "            gray_img = cv2.cvtColor((frame * 255).astype(np.uint8),cv2.COLOR_RGB2GRAY)\n",
        "            normalized_frame = gray_img / 255.0\n",
        "\n",
        "             # Expand dimensions to match the required channel size (e.g., 3 for RGB)\n",
        "            expanded_frame = np.expand_dims(normalized_frame, axis=-1)  # Add a channel dimension\n",
        "\n",
        "            # Duplicate the grayscale channel to match the number of channels in the original RGB frames\n",
        "            stacked_frame = np.concatenate([expanded_frame] * 3, axis=-1)  # Repeat grayscale channel 3 times\n",
        "\n",
        "            frame_list_aug.append(stacked_frame)\n",
        "\n",
        "        X_train_aug.append(np.asarray(frame_list_aug))\n",
        "\n",
        "    for label in Y_train:\n",
        "        Y_train_aug.append(label)\n",
        "\n",
        "    return X_train_aug,Y_train_aug"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uu1okiseyKpj",
      "metadata": {
        "id": "Uu1okiseyKpj"
      },
      "source": [
        "### Color Inverted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QsJ_SvL1x5bF",
      "metadata": {
        "id": "QsJ_SvL1x5bF"
      },
      "outputs": [],
      "source": [
        "def data_augmentation_inverted(X_train,Y_train):\n",
        "    X_train_aug = []\n",
        "    Y_train_aug = []\n",
        "\n",
        "    for frame_list in X_train:\n",
        "        frame_list_aug = []\n",
        "        for frame in frame_list:\n",
        "            invert_img =  cv2.bitwise_not((frame * 255).astype(np.uint8))\n",
        "            normalized_frame = invert_img / 255.0\n",
        "\n",
        "            frame_list_aug.append(normalized_frame)\n",
        "\n",
        "        X_train_aug.append(np.asarray(frame_list_aug))\n",
        "\n",
        "    for label in Y_train:\n",
        "        Y_train_aug.append(label)\n",
        "\n",
        "    return X_train_aug,Y_train_aug"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eC15jKmAyRkq",
      "metadata": {
        "id": "eC15jKmAyRkq"
      },
      "source": [
        "Data augmentation and combine with existing training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121cbcbd-02c7-4994-884e-d20f1b5f3bc2",
      "metadata": {
        "id": "121cbcbd-02c7-4994-884e-d20f1b5f3bc2"
      },
      "outputs": [],
      "source": [
        "X_train_aug_1,Y_train_aug_1 = data_augmentation_flip_bright(X_train,Y_train)\n",
        "X_train_aug_2,Y_train_aug_2 = data_augmentation_rotate_contrast(X_train,Y_train)\n",
        "X_train_aug_3,Y_train_aug_3 = data_augmentation_grayscale(X_train,Y_train)\n",
        "\n",
        "X_train_aug_1 = np.array([np.array(frame_list) for frame_list in X_train_aug_1])\n",
        "X_train_aug_2 = np.array([np.array(frame_list) for frame_list in X_train_aug_2])\n",
        "X_train_aug_3 = np.array([np.array(frame_list) for frame_list in X_train_aug_3])\n",
        "\n",
        "X_train = np.append(X_train, X_train_aug_1, axis=0)\n",
        "Y_train = np.append(Y_train, Y_train_aug_1, axis=0)\n",
        "X_train = np.append(X_train, X_train_aug_2, axis=0)\n",
        "Y_train = np.append(Y_train, Y_train_aug_2, axis=0)\n",
        "X_train = np.append(X_train, X_train_aug_3, axis=0)\n",
        "Y_train = np.append(Y_train, Y_train_aug_3, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qS3LFUwqze5K",
      "metadata": {
        "id": "qS3LFUwqze5K"
      },
      "source": [
        "## Data augmentation visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lEpLzdhvzllj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "lEpLzdhvzllj",
        "outputId": "efdf3bd8-fca1-4d8b-e0f7-2cb2b69fe523"
      },
      "outputs": [],
      "source": [
        "ori_images = np.clip(X_train[2][1]*255, 0, 255).astype(np.uint8)\n",
        "plt.imshow(ori_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qys4jLZmz0As",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "qys4jLZmz0As",
        "outputId": "2e877762-08c3-4eba-dde2-999e99bbcddf"
      },
      "outputs": [],
      "source": [
        "flip_images = np.clip(X_train_aug_1[2][1]*255, 0, 255).astype(np.uint8)\n",
        "plt.imshow(flip_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WI0yFDZMz8He",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "WI0yFDZMz8He",
        "outputId": "09ac0150-6756-4a11-8640-9570eb424070"
      },
      "outputs": [],
      "source": [
        "rotate_images = np.clip(X_train_aug_2[2][1]*255, 0, 255).astype(np.uint8)\n",
        "plt.imshow(rotate_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RDo8IW3o0DWg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "RDo8IW3o0DWg",
        "outputId": "5a19e86b-1cc9-45ed-b8e6-1d5463445dec"
      },
      "outputs": [],
      "source": [
        "gray_images = np.clip(X_train_aug_3[2][1]*255, 0, 255).astype(np.uint8)\n",
        "plt.imshow(gray_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a94b78b-2cca-4a4b-b258-41d0fbe84e0e",
      "metadata": {
        "id": "7a94b78b-2cca-4a4b-b258-41d0fbe84e0e"
      },
      "source": [
        "# Construct Model - Using Pytorch RestNet50\n",
        "To avoid the overfitting issue, the model will apply dropout, regularization and batch normalization technique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40663196-29b8-434d-a1fc-9785c17ca6cd",
      "metadata": {
        "id": "40663196-29b8-434d-a1fc-9785c17ca6cd"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "\n",
        "class ResNet50_LSTM(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super(ResNet50_LSTM, self).__init__()\n",
        "\n",
        "        weights = ResNet50_Weights.DEFAULT\n",
        "        self.resnet = resnet50(weights=weights)\n",
        "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=2048, hidden_size=256, num_layers=1, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, timesteps, H, W, C = x.size()\n",
        "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
        "\n",
        "        c_out = self.resnet(c_in)\n",
        "        r_out = c_out.view(batch_size, timesteps, -1)\n",
        "        r_out, (hn, cn) = self.lstm(r_out)\n",
        "\n",
        "        r_out = self.dropout(r_out[:, -1, :])\n",
        "        out = self.fc(r_out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad8d96d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "Y_train_indices = np.argmax(Y_train, axis=1)\n",
        "Y_validate_indices = np.argmax(Y_validate, axis=1)\n",
        "Y_test_indices = np.argmax(Y_test, axis=1)\n",
        "\n",
        "\n",
        "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train_indices, dtype=torch.long))\n",
        "validate_data = TensorDataset(torch.tensor(X_validate, dtype=torch.float32), torch.tensor(Y_validate_indices, dtype=torch.long))\n",
        "test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test_indices, dtype=torch.long))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "validate_loader = DataLoader(validate_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc1586f",
      "metadata": {},
      "source": [
        "## Model training\n",
        "To avoid overfitting issue, reduce LR technique will be utilised."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca56197c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0edba3d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07e83bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# reset_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(class_list)\n",
        "model = ResNet50_LSTM(num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=0.001)\n",
        "\n",
        "num_epochs = 20\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "valid_accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = validate(model, validate_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6632bcd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    pred_list =[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred = output.argmax(dim=1)\n",
        "            pred_list.append(pred.tolist())\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy,pred_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "286d4d31",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def get_all_preds_and_labels(model, data_loader, device):\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# 使用测试集获取预测和标签\n",
        "preds, labels = get_all_preds_and_labels(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad399b7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(labels, preds)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e5698e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "test_loss, test_accuracy, y_pred= evaluate(model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb1cd2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bfa1ea2",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(valid_accuracies, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068eb3d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(valid_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
